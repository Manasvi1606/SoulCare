{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9549501-644f-4316-9f11-2a49cbae61f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in c:\\programdata\\anaconda3\\lib\\site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56006ce-fd8a-4c15-b6d9-316451956e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (10.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2611624a-a953-488b-bd8e-bbb9bf22f054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing labriries\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492315c3-5ce6-4282-aa21-8d9eba96f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disable the use of OpenCL in OpenCV\n",
    "cv2.ocl.setUseOpenCL(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835fc2f4-4f64-451d-87b2-6b275e8d9500",
   "metadata": {},
   "source": [
    "## preparing the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abb54112-9c50-44de-bc6b-c2237fe62690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23769 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "#flow_from_directory isusing for a data like the one i have with diffrent file for each class\n",
    "train_generator = ImageDataGenerator(rescale=1./255,rotation_range=20,zoom_range=0.1,width_shift_range=0.1,height_shift_range=0.1, validation_split=0.2).flow_from_directory(\n",
    "        'data2/train',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical',subset='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7726a2c3-7401-4aac-ac3c-24141453d5d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## preparing the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e60dd89b-ffa2-48b4-a059-93663bc7f383",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5941 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = ImageDataGenerator(rescale=1./255,rotation_range=20,zoom_range=0.1,width_shift_range=0.1,height_shift_range=0.1,validation_split=0.2).flow_from_directory(\n",
    "        'data2/train',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical',subset='validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b2bcc4-c3f8-48d3-a143-dc18ef591edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0.00005,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b515f69a-1292-4f80-b9c3-a8113300bae9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## building the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6700428c-6704-4d77-b0f7-aee6eb0e91b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_mental_health = Sequential([#features extraction phase\n",
    "layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)),#conv2D convolution over images/input with 1 color(greyscale)\n",
    "layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),#relu is activation function that turn the negative to zero\n",
    "layers.MaxPooling2D(pool_size=(2, 2)),#maxpooling2d condensing the data (take  the max from the convolution map) to reduice the size\n",
    "layers.Dropout(0.25),#protect from overfitting\n",
    "layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "layers.Dropout(0.25),\n",
    "layers.Flatten(),#flatten is preparing the data to the neural network\n",
    "layers.Dense(1024, activation='relu'),#1024 number of nerons\n",
    "layers.Dropout(0.5),\n",
    "layers.Dense(7, activation='softmax')])#7units corresponding to the number of classes\n",
    "#softmax is used to normalize the output and gives probabilities of each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "805d3c48-1947-4111-9e46-ebf03d53beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mental_health.compile(loss='categorical_crossentropy', optimizer=Adam( learning_rate=0.0001), metrics=['accuracy'])\n",
    "#categorical_crossentropy used for multi-class classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a17f855-c59c-4ef0-be1d-29a5caad0f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 46, 46, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 44, 44, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 22, 22, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 22, 22, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 20, 20, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 10, 10, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 7175      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2345607 (8.95 MB)\n",
      "Trainable params: 2345607 (8.95 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_mental_health.summary()# to show the steps of the model for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6aa379-dc79-4eb0-8a89-e38509fc819b",
   "metadata": {},
   "source": [
    "## fitting the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc05ee85-8ed5-48a6-abcd-f553393f560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "371/371 [==============================] - 56s 152ms/step - loss: 1.6893 - accuracy: 0.3242 - val_loss: 1.6573 - val_accuracy: 0.3407\n",
      "Epoch 2/100\n",
      "371/371 [==============================] - 58s 157ms/step - loss: 1.6382 - accuracy: 0.3539 - val_loss: 1.6184 - val_accuracy: 0.3635\n",
      "Epoch 3/100\n",
      "371/371 [==============================] - 57s 154ms/step - loss: 1.5973 - accuracy: 0.3704 - val_loss: 1.5678 - val_accuracy: 0.3779\n",
      "Epoch 4/100\n",
      "371/371 [==============================] - 57s 154ms/step - loss: 1.5561 - accuracy: 0.3905 - val_loss: 1.5233 - val_accuracy: 0.4069\n",
      "Epoch 5/100\n",
      "371/371 [==============================] - 57s 152ms/step - loss: 1.5188 - accuracy: 0.4096 - val_loss: 1.4979 - val_accuracy: 0.4175\n",
      "Epoch 6/100\n",
      "371/371 [==============================] - 57s 155ms/step - loss: 1.4863 - accuracy: 0.4239 - val_loss: 1.4490 - val_accuracy: 0.4446\n",
      "Epoch 7/100\n",
      "371/371 [==============================] - 58s 156ms/step - loss: 1.4525 - accuracy: 0.4462 - val_loss: 1.4303 - val_accuracy: 0.4494\n",
      "Epoch 8/100\n",
      "371/371 [==============================] - 57s 153ms/step - loss: 1.4235 - accuracy: 0.4555 - val_loss: 1.3904 - val_accuracy: 0.4711\n",
      "Epoch 9/100\n",
      "371/371 [==============================] - 56s 151ms/step - loss: 1.3954 - accuracy: 0.4696 - val_loss: 1.3830 - val_accuracy: 0.4742\n",
      "Epoch 10/100\n",
      "371/371 [==============================] - 58s 156ms/step - loss: 1.3755 - accuracy: 0.4720 - val_loss: 1.3566 - val_accuracy: 0.4801\n",
      "Epoch 11/100\n",
      "371/371 [==============================] - 57s 154ms/step - loss: 1.3580 - accuracy: 0.4806 - val_loss: 1.3610 - val_accuracy: 0.4767\n",
      "Epoch 12/100\n",
      "371/371 [==============================] - 59s 160ms/step - loss: 1.3362 - accuracy: 0.4942 - val_loss: 1.3082 - val_accuracy: 0.5071\n",
      "Epoch 13/100\n",
      "371/371 [==============================] - 69s 186ms/step - loss: 1.3206 - accuracy: 0.5000 - val_loss: 1.3073 - val_accuracy: 0.5042\n",
      "Epoch 14/100\n",
      "371/371 [==============================] - 59s 158ms/step - loss: 1.3100 - accuracy: 0.5028 - val_loss: 1.3025 - val_accuracy: 0.5046\n",
      "Epoch 15/100\n",
      "371/371 [==============================] - 64s 174ms/step - loss: 1.2929 - accuracy: 0.5096 - val_loss: 1.2888 - val_accuracy: 0.5126\n",
      "Epoch 16/100\n",
      "371/371 [==============================] - 64s 172ms/step - loss: 1.2842 - accuracy: 0.5093 - val_loss: 1.2724 - val_accuracy: 0.5222\n",
      "Epoch 17/100\n",
      "371/371 [==============================] - 59s 158ms/step - loss: 1.2719 - accuracy: 0.5152 - val_loss: 1.2673 - val_accuracy: 0.5195\n",
      "Epoch 18/100\n",
      "371/371 [==============================] - 59s 160ms/step - loss: 1.2582 - accuracy: 0.5254 - val_loss: 1.2606 - val_accuracy: 0.5231\n",
      "Epoch 19/100\n",
      "371/371 [==============================] - 58s 157ms/step - loss: 1.2527 - accuracy: 0.5278 - val_loss: 1.2485 - val_accuracy: 0.5304\n",
      "Epoch 20/100\n",
      "371/371 [==============================] - 59s 158ms/step - loss: 1.2412 - accuracy: 0.5298 - val_loss: 1.2465 - val_accuracy: 0.5287\n",
      "Epoch 21/100\n",
      "371/371 [==============================] - 59s 158ms/step - loss: 1.2334 - accuracy: 0.5325 - val_loss: 1.2312 - val_accuracy: 0.5329\n",
      "Epoch 22/100\n",
      "371/371 [==============================] - 59s 159ms/step - loss: 1.2207 - accuracy: 0.5344 - val_loss: 1.2341 - val_accuracy: 0.5369\n",
      "Epoch 23/100\n",
      "371/371 [==============================] - 58s 157ms/step - loss: 1.2161 - accuracy: 0.5374 - val_loss: 1.2185 - val_accuracy: 0.5348\n",
      "Epoch 24/100\n",
      "371/371 [==============================] - 59s 160ms/step - loss: 1.2021 - accuracy: 0.5452 - val_loss: 1.2135 - val_accuracy: 0.5416\n",
      "Epoch 25/100\n",
      "371/371 [==============================] - 60s 162ms/step - loss: 1.1977 - accuracy: 0.5458 - val_loss: 1.1992 - val_accuracy: 0.5447\n",
      "Epoch 26/100\n",
      "371/371 [==============================] - 60s 160ms/step - loss: 1.1910 - accuracy: 0.5522 - val_loss: 1.2027 - val_accuracy: 0.5506\n",
      "Epoch 27/100\n",
      "371/371 [==============================] - 58s 157ms/step - loss: 1.1859 - accuracy: 0.5517 - val_loss: 1.1953 - val_accuracy: 0.5470\n",
      "Epoch 28/100\n",
      "371/371 [==============================] - 58s 158ms/step - loss: 1.1836 - accuracy: 0.5536 - val_loss: 1.1723 - val_accuracy: 0.5513\n",
      "Epoch 29/100\n",
      "371/371 [==============================] - 59s 159ms/step - loss: 1.1619 - accuracy: 0.5595 - val_loss: 1.1880 - val_accuracy: 0.5443\n",
      "Epoch 30/100\n",
      "371/371 [==============================] - 59s 159ms/step - loss: 1.1657 - accuracy: 0.5617 - val_loss: 1.1879 - val_accuracy: 0.5455\n",
      "Epoch 31/100\n",
      "371/371 [==============================] - 62s 166ms/step - loss: 1.1540 - accuracy: 0.5661 - val_loss: 1.1640 - val_accuracy: 0.5550\n",
      "Epoch 32/100\n",
      "371/371 [==============================] - 60s 163ms/step - loss: 1.1518 - accuracy: 0.5648 - val_loss: 1.1676 - val_accuracy: 0.5547\n",
      "Epoch 33/100\n",
      "371/371 [==============================] - 59s 159ms/step - loss: 1.1467 - accuracy: 0.5680 - val_loss: 1.1654 - val_accuracy: 0.5566\n",
      "Epoch 34/100\n",
      "371/371 [==============================] - 58s 155ms/step - loss: 1.1417 - accuracy: 0.5703 - val_loss: 1.1508 - val_accuracy: 0.5640\n",
      "Epoch 35/100\n",
      "371/371 [==============================] - 58s 157ms/step - loss: 1.1362 - accuracy: 0.5707 - val_loss: 1.1567 - val_accuracy: 0.5637\n",
      "Epoch 36/100\n",
      "371/371 [==============================] - 64s 171ms/step - loss: 1.1283 - accuracy: 0.5738 - val_loss: 1.1471 - val_accuracy: 0.5679\n",
      "Epoch 37/100\n",
      "371/371 [==============================] - 60s 161ms/step - loss: 1.1250 - accuracy: 0.5755 - val_loss: 1.1595 - val_accuracy: 0.5526\n",
      "Epoch 38/100\n",
      "371/371 [==============================] - 55s 148ms/step - loss: 1.1227 - accuracy: 0.5781 - val_loss: 1.1382 - val_accuracy: 0.5732\n",
      "Epoch 39/100\n",
      "371/371 [==============================] - 55s 147ms/step - loss: 1.1059 - accuracy: 0.5821 - val_loss: 1.1416 - val_accuracy: 0.5666\n",
      "Epoch 40/100\n",
      "371/371 [==============================] - 55s 149ms/step - loss: 1.1056 - accuracy: 0.5829 - val_loss: 1.1411 - val_accuracy: 0.5667\n",
      "Epoch 41/100\n",
      "371/371 [==============================] - 60s 163ms/step - loss: 1.1017 - accuracy: 0.5865 - val_loss: 1.1334 - val_accuracy: 0.5732\n",
      "Epoch 42/100\n",
      "371/371 [==============================] - 64s 173ms/step - loss: 1.0967 - accuracy: 0.5854 - val_loss: 1.1317 - val_accuracy: 0.5693\n",
      "Epoch 43/100\n",
      "371/371 [==============================] - 59s 160ms/step - loss: 1.0950 - accuracy: 0.5843 - val_loss: 1.1367 - val_accuracy: 0.5693\n",
      "Epoch 44/100\n",
      "371/371 [==============================] - 60s 163ms/step - loss: 1.0884 - accuracy: 0.5898 - val_loss: 1.1242 - val_accuracy: 0.5751\n",
      "Epoch 45/100\n",
      "371/371 [==============================] - 59s 160ms/step - loss: 1.0870 - accuracy: 0.5937 - val_loss: 1.1215 - val_accuracy: 0.5786\n",
      "Epoch 46/100\n",
      "371/371 [==============================] - 71s 190ms/step - loss: 1.0785 - accuracy: 0.5943 - val_loss: 1.1316 - val_accuracy: 0.5666\n",
      "Epoch 47/100\n",
      "371/371 [==============================] - 58s 157ms/step - loss: 1.0683 - accuracy: 0.5992 - val_loss: 1.1156 - val_accuracy: 0.5844\n",
      "Epoch 48/100\n",
      "371/371 [==============================] - 56s 151ms/step - loss: 1.0666 - accuracy: 0.5976 - val_loss: 1.1113 - val_accuracy: 0.5875\n",
      "Epoch 49/100\n",
      "371/371 [==============================] - 56s 151ms/step - loss: 1.0647 - accuracy: 0.6047 - val_loss: 1.1181 - val_accuracy: 0.5795\n",
      "Epoch 50/100\n",
      "371/371 [==============================] - 56s 151ms/step - loss: 1.0661 - accuracy: 0.5989 - val_loss: 1.1091 - val_accuracy: 0.5814\n",
      "Epoch 51/100\n",
      "371/371 [==============================] - 56s 152ms/step - loss: 1.0576 - accuracy: 0.6019 - val_loss: 1.1033 - val_accuracy: 0.5885\n",
      "Epoch 52/100\n",
      "371/371 [==============================] - 56s 151ms/step - loss: 1.0508 - accuracy: 0.6059 - val_loss: 1.1001 - val_accuracy: 0.5864\n",
      "Epoch 53/100\n",
      "371/371 [==============================] - 55s 149ms/step - loss: 1.0467 - accuracy: 0.6095 - val_loss: 1.0922 - val_accuracy: 0.5897\n",
      "Epoch 54/100\n",
      "371/371 [==============================] - 55s 148ms/step - loss: 1.0431 - accuracy: 0.6081 - val_loss: 1.0980 - val_accuracy: 0.5841\n",
      "Epoch 55/100\n",
      "371/371 [==============================] - 55s 147ms/step - loss: 1.0384 - accuracy: 0.6119 - val_loss: 1.0983 - val_accuracy: 0.5873\n",
      "Epoch 56/100\n",
      "371/371 [==============================] - 55s 149ms/step - loss: 1.0381 - accuracy: 0.6097 - val_loss: 1.0912 - val_accuracy: 0.5912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "371/371 [==============================] - 55s 147ms/step - loss: 1.0373 - accuracy: 0.6133 - val_loss: 1.0816 - val_accuracy: 0.5941\n",
      "Epoch 58/100\n",
      "371/371 [==============================] - 54s 146ms/step - loss: 1.0320 - accuracy: 0.6133 - val_loss: 1.0995 - val_accuracy: 0.5892\n",
      "Epoch 59/100\n",
      "371/371 [==============================] - 55s 148ms/step - loss: 1.0231 - accuracy: 0.6146 - val_loss: 1.0866 - val_accuracy: 0.5985\n",
      "Epoch 60/100\n",
      "371/371 [==============================] - 54s 147ms/step - loss: 1.0254 - accuracy: 0.6158 - val_loss: 1.0869 - val_accuracy: 0.5939\n",
      "Epoch 61/100\n",
      "371/371 [==============================] - 54s 146ms/step - loss: 1.0168 - accuracy: 0.6187 - val_loss: 1.0931 - val_accuracy: 0.5919\n",
      "Epoch 62/100\n",
      "371/371 [==============================] - 54s 147ms/step - loss: 1.0111 - accuracy: 0.6222 - val_loss: 1.0906 - val_accuracy: 0.5919\n",
      "Epoch 63/100\n",
      "371/371 [==============================] - 55s 149ms/step - loss: 1.0083 - accuracy: 0.6220 - val_loss: 1.0833 - val_accuracy: 0.5856\n",
      "Epoch 64/100\n",
      "371/371 [==============================] - 55s 147ms/step - loss: 1.0035 - accuracy: 0.6252 - val_loss: 1.0855 - val_accuracy: 0.5910\n",
      "Epoch 65/100\n",
      "371/371 [==============================] - 55s 147ms/step - loss: 1.0063 - accuracy: 0.6240 - val_loss: 1.0891 - val_accuracy: 0.5910\n",
      "Epoch 66/100\n",
      "371/371 [==============================] - 55s 148ms/step - loss: 0.9949 - accuracy: 0.6288 - val_loss: 1.0852 - val_accuracy: 0.5897\n",
      "Epoch 67/100\n",
      "371/371 [==============================] - 56s 151ms/step - loss: 1.0018 - accuracy: 0.6230 - val_loss: 1.0716 - val_accuracy: 0.5951\n",
      "Epoch 68/100\n",
      "371/371 [==============================] - 60s 162ms/step - loss: 0.9893 - accuracy: 0.6311 - val_loss: 1.0822 - val_accuracy: 0.5890\n",
      "Epoch 69/100\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.9852 - accuracy: 0.6308Restoring model weights from the end of the best epoch: 59.\n",
      "371/371 [==============================] - 61s 164ms/step - loss: 0.9852 - accuracy: 0.6308 - val_loss: 1.0754 - val_accuracy: 0.5931\n",
      "Epoch 69: early stopping\n"
     ]
    }
   ],
   "source": [
    "my_mental_health_info = my_mental_health.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // 64,\n",
    "        epochs=100,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=val_generator.samples // 64,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5f7c3-c2a4-43c6-b65f-828fe463d6c4",
   "metadata": {},
   "source": [
    "\n",
    "## make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eff3bef-9632-4136-95af-51179d97eba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = ImageDataGenerator(rescale=1./255,rotation_range=20,zoom_range=0.1,width_shift_range=0.1,height_shift_range=0.1).flow_from_directory(\n",
    "        'data2/test',\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b8b2734-6d0a-4c03-a4d8-f3f7352a8973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 40s 362ms/step - loss: 1.0749 - accuracy: 0.5921\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = my_mental_health.evaluate(test_generator, steps= test_generator.samples// 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ba163-0723-4425-b886-8c987bef8a2c",
   "metadata": {},
   "source": [
    "## check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e29ff86-2f9f-46b2-8beb-17501be1067d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0749\n",
      "Test Accuracy: 0.5921\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6dacbbc-0da6-4f8d-b2b5-dace900369e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 7s 59ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = my_mental_health.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a130a1a1-a7ca-4817-9863-b824e34f33e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5913903594315966\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted_labels = np.argmax(predictions, axis=1)  # Get the index of the predicted class with highest probability\n",
    "true_labels = test_generator.classes  # Get the true labels of the test samples\n",
    "\n",
    "accuracy = np.mean(predicted_labels == true_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db4279-3ba8-4b1b-ab89-3dbb41ae62fc",
   "metadata": {},
   "source": [
    "## save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba440398-8a6c-4879-9a9c-4eba482471fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m my_mental_health\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhealth2.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "my_mental_health.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868fb0c-a233-4333-8fb8-148c3e0ea187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
